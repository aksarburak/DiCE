{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick introduction to generating counterfactual explanations using DiCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import DiCE\n",
    "import dice_ml\n",
    "from dice_ml.utils import helpers # helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DiCE requires two inputs: a training dataset and a pre-trained ML model. It can also work without access to the full dataset (see this [notebook](DiCE_with_private_data.ipynb) for advanced examples)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the \"adult\" income dataset from UCI Machine Learning Repository (https://archive.ics.uci.edu/ml/datasets/adult). For demonstration purposes, we transform the data as described in **dice_ml.utils.helpers** module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = helpers.load_adult_income_dataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset has 8 features. The outcome is income which is binarized to 0 (low-income, <=50K) or 1 (high-income, >50K). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# description of transformed features\n",
    "adult_info = helpers.get_adult_data_info()\n",
    "adult_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given this dataset, we construct a data object for DiCE. Since continuous and discrete features have different ways of perturbation, we need to specify the names of the continuous features. DiCE also requires the name of the output variable that the ML model will predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = dice_ml.Data(dataframe=dataset, continuous_features=['age', 'hours_per_week'], outcome_name='income')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the ML model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, we use a pre-trained ML model which produces high accuracy comparable to other baselines. For convenience, we include the sample trained model with the DiCE package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.utils.data\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import save_image\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from dice_ml.model_interfaces.pytorch_model import PyTorchModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp_shape= len(d.encoded_feature_names)\n",
    "\n",
    "ML_modelpath = helpers.get_adult_income_modelpath()\n",
    "m = PyTorchModel(inp_shape)\n",
    "\n",
    "learning_rate = 0.001\n",
    "# Default Batch Size of Keras\n",
    "batch_size = 32\n",
    "optimizer = optim.Adam([\n",
    "    {'params': filter(lambda p: p.requires_grad, m.ann_model.parameters()) }\n",
    "], lr=learning_rate)\n",
    "crieterion= nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pre Trained\n",
    "base_model_dir= '../dice_ml/utils/sample_trained_models/'\n",
    "dataset_name= 'adult'\n",
    "path=base_model_dir+dataset_name+'-pytorch.pth'\n",
    "m.load_state_dict(torch.load(path))\n",
    "m.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset for training Black Box Model\n",
    "train_data_vae= d.data_df.copy()\n",
    "\n",
    "#Creating list of encoded categorical and continuous feature indices\n",
    "encoded_categorical_feature_indexes = d.get_data_params()[2]     \n",
    "encoded_continuous_feature_indexes=[]\n",
    "data_size= len(d.encoded_feature_names)\n",
    "for i in range(data_size):\n",
    "    valid=1\n",
    "    for v in encoded_categorical_feature_indexes:\n",
    "        if i in v:\n",
    "            valid=0\n",
    "    if valid:\n",
    "        encoded_continuous_feature_indexes.append(i)            \n",
    "encoded_start_cat = len(encoded_continuous_feature_indexes)\n",
    "        \n",
    "#One Hot Encoding for categorical features\n",
    "encoded_data = d.one_hot_encode_data(train_data_vae)\n",
    "\n",
    "# The output/outcome variable position altered due to one_hot_encoding for categorical features: (Cont feat, Outcome, Cat feat) \n",
    "# Need to rearrange columns such that outcome variable comes at the last\n",
    "cols = list(encoded_data.columns)\n",
    "cols = cols[:encoded_start_cat] + cols[encoded_start_cat+1:] + [cols[encoded_start_cat]]\n",
    "encoded_data = encoded_data[cols]     \n",
    "\n",
    "#Normlization for conitnuous features\n",
    "encoded_data= d.normalize_data(encoded_data)\n",
    "print(encoded_data.columns)\n",
    "dataset = encoded_data.to_numpy()\n",
    "\n",
    "#Train, Val, Test Splits\n",
    "np.random.shuffle(dataset)\n",
    "test_size= int(0.2*dataset.shape[0])\n",
    "val_dataset= dataset[:test_size]\n",
    "train_dataset= dataset[test_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training\n",
    "for epoch in range(50):\n",
    "    np.random.shuffle(train_dataset)\n",
    "    train_batches= np.array_split( train_dataset, train_dataset.shape[0]//batch_size ,axis=0 )    \n",
    "    print('Epoch: ', epoch)\n",
    "    train_acc=0.0\n",
    "    for i in range(len(train_batches)):    \n",
    "        optimizer.zero_grad()\n",
    "        train_x= torch.tensor( train_batches[i][:,:-1] ).float() \n",
    "        train_y= torch.tensor( train_batches[i][:,-1], dtype=torch.int64 )\n",
    "        \n",
    "        out= m(train_x)\n",
    "        train_acc += torch.sum( torch.argmax(out, axis=1) == train_y )\n",
    "        \n",
    "        # Cross Entropy Loss\n",
    "        loss= crieterion(out, train_y)\n",
    "        #L2 Regularization\n",
    "        weight_norm = torch.tensor(0.)\n",
    "        for w in m.ann_model.parameters():\n",
    "            weight_norm += w.norm().pow(1)\n",
    "        loss+= 0.001*weight_norm\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(train_acc, len(train_dataset))     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation        \n",
    "np.random.shuffle(val_dataset)\n",
    "train_batches= np.array_split( val_dataset, val_dataset.shape[0]//batch_size ,axis=0 )    \n",
    "val_acc=0.0\n",
    "for i in range(len(train_batches)):    \n",
    "    optimizer.zero_grad()\n",
    "    train_x= torch.tensor( train_batches[i][:,:-1] ).float() \n",
    "    train_y= torch.tensor( train_batches[i][:,-1], dtype=torch.int64 )\n",
    "    out= m(train_x)\n",
    "    val_acc += torch.sum( torch.argmax(out, axis=1) == train_y )\n",
    "print(val_acc, len(val_dataset))\t\n",
    "\n",
    "#Saving the Black Box Model\n",
    "base_model_dir= '../dice_ml/utils/sample_trained_models/'\n",
    "dataset_name= 'adult'\n",
    "path=base_model_dir+dataset_name+'-pytorch.pth'\n",
    "torch.save(m.state_dict(), path)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For an example of how to train your own model, check out [this](DiCE_with_advanced_options.ipynb) notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate feasible counterfactuals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the data object *d* and the model object *m*, we can now instantiate the DiCE class for generating explanations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from dice_ml.dice_interfaces.dice_base_gencf import DiceBaseGenCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate DiCE\n",
    "exp = DiceBaseGenCF(d, m)\n",
    "exp.train(pre_trained=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DiCE is a form of a local explanation and requires an query input whose outcome needs to be explained. Below we provide a sample input whose outcome is 0 (low-income) as per the ML model object *m*. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query instance in the form of a dictionary; keys: feature name, values: feature value\n",
    "query_instance = {'age':22, \n",
    "                  'workclass':'Private', \n",
    "                  'education':'HS-grad', \n",
    "                  'marital_status':'Single', \n",
    "                  'occupation':'Service',\n",
    "                  'race': 'White', \n",
    "                  'gender':'Female', \n",
    "                  'hours_per_week': 45}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the query input, we can now generate counterfactual explanations to show perturbed inputs from the original input where the ML model outputs class 1 (high-income). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate counterfactuals\n",
    "dice_exp = exp.generate_counterfactuals(query_instance, total_CFs=15, desired_class=\"opposite\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# visualize the results\n",
    "dice_exp.visualize_as_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! You can try generating counterfactual explanations for other examples using the same code. You can also try out a use-case for sensitive data in [this](DiCE_with_private_data.ipynb) notebook. \n",
    "\n",
    "If you are curious about changing the number of explanations showns, feasibility of the explanations, or how to weigh different features for perturbing, check out how to change DiCE's behavior in this [advanced notebook](DiCE_with_advanced_options.ipynb).  \n",
    "\n",
    "The counterfactuals generated above are slightly different from those shown in [our paper](https://arxiv.org/pdf/1905.07697.pdf), where the loss convergence condition was made more conservative for rigorous experimentation. To replicate the results in the paper, add an argument *loss_converge_maxiter=2* (the default value is 1) in the *exp.generate_counterfactuals()* method above. For more info, see *generate_counterfactuals()* method in [dice_ml.dice_interfaces.dice_tensorflow.py](../dice_ml/dice_interfaces/dice_tensorflow.py)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ModelApprox "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dice_ml.dice_interfaces.dice_model_approx_gencf import DiceModelApproxGenCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initiate DiCE\n",
    "exp = DiceModelApproxGenCF(d, m)\n",
    "exp.train(1, [[0]], 1, 10, pre_trained=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate counterfactuals\n",
    "dice_exp = exp.generate_counterfactuals(query_instance, total_CFs=15, desired_class=\"opposite\")\n",
    "# visualize the results\n",
    "dice_exp.visualize_as_dataframe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dice_env",
   "language": "python",
   "name": "dice_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
